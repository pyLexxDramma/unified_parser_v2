# Предложение по оптимизации: ранняя фильтрация по сайту

## Текущая логика (неоптимальная)

### Яндекс:
1. Найти все карточки (`_get_links()` или через скролл)
2. Для каждой карточки:
   - Загрузить детальную страницу
   - Извлечь все данные (название, адрес, рейтинг, **сайт**, телефон)
   - **Загрузить и распарсить ВСЕ отзывы** (`_get_card_reviews_info()`) - ДОЛГО!
3. После парсинга всех карточек - фильтровать по сайту

### 2GIS:
1. Найти все карточки (`_get_links()`)
2. Для каждой карточки:
   - Загрузить страницу карточки
   - Извлечь все данные (название, адрес, рейтинг, **сайт**, телефон)
   - **Загрузить и распарсить ВСЕ отзывы** (`_get_card_reviews_info_2gis()`) - ДОЛГО!
3. После парсинга всех карточек - фильтровать по сайту

## Проблема

Если из 50 найденных карточек только 2 имеют нужный сайт, мы:
- Парсим отзывы для всех 50 карточек (очень долго!)
- Потом отбрасываем 48 карточек

## Предлагаемая оптимизация

### Новая логика:

1. Найти все карточки (URL)
2. **БЫСТРАЯ ПРОВЕРКА**: Для каждой карточки:
   - Загрузить страницу карточки
   - Извлечь **ТОЛЬКО сайт** (быстро, без парсинга отзывов)
   - Проверить соответствие целевому сайту
3. **ФИЛЬТРАЦИЯ**: Оставить только карточки с нужным сайтом
4. **ПОЛНЫЙ ПАРСИНГ**: Только для отфильтрованных карточек:
   - Извлечь все данные
   - Парсить отзывы

## Преимущества

1. **Экономия времени**: Не парсим отзывы для ненужных карточек
2. **Экономия ресурсов**: Меньше запросов к серверам
3. **Быстрее результат**: Пользователь получает данные быстрее
4. **Меньше нагрузка**: Меньше нагрузка на Яндекс/2ГИС

## Реализация

### Для Яндекс:
```python
def _quick_extract_website(self, card_url: str) -> str:
    """Быстро извлекает только сайт из карточки без полного парсинга"""
    self.driver.navigate(card_url)
    time.sleep(1)  # Минимальная задержка
    page_source = self.driver.get_page_source()
    soup = BeautifulSoup(page_source, "lxml")
    
    website_selectors = [
        'a[itemprop="url"]',
        '.business-website-view__link',
        'a[href^="http"]:not([href*="yandex.ru"])',
    ]
    for selector in website_selectors:
        elem = soup.select_one(selector)
        if elem:
            href = elem.get('href', '')
            if href and 'yandex.ru' not in href.lower():
                return href
    return ""

def _parse_cards_with_early_filtering(self, url: str, target_website: str = None):
    # 1. Найти все карточки
    card_urls = self._get_all_card_urls(url)
    
    # 2. Быстрая фильтрация по сайту (если указан)
    if target_website:
        filtered_urls = []
        for card_url in card_urls:
            website = self._quick_extract_website(card_url)
            if self._website_matches(website, target_website):
                filtered_urls.append(card_url)
        card_urls = filtered_urls
    
    # 3. Полный парсинг только для отфильтрованных карточек
    for card_url in card_urls:
        # Полный парсинг с отзывами
        card_data = self._extract_card_data_from_detail_page(...)
        reviews = self._get_card_reviews_info()
        # ...
```

### Для 2GIS:
Аналогично - добавить метод `_quick_extract_website_2gis()` и фильтровать до `_get_card_reviews_info_2gis()`

## Оценка экономии

Если из 50 карточек только 2 нужны:
- **Текущий подход**: Парсим отзывы для 50 карточек = ~50 минут
- **Оптимизированный**: Быстрая проверка 50 карточек (~5 минут) + парсинг отзывов для 2 карточек (~2 минуты) = ~7 минут
- **Экономия**: ~43 минуты (86% времени!)

## Риски

1. **Сайт может быть не на странице карточки**: Нужно проверить, всегда ли сайт доступен без полной загрузки
2. **Дополнительные запросы**: Но они быстрые (только загрузка страницы, без парсинга отзывов)
3. **Изменение структуры сайта**: Селекторы могут измениться, но это актуально и для текущего подхода

## Рекомендация

**Внедрить оптимизацию**, так как:
- Значительная экономия времени
- Меньше нагрузка на сервисы
- Лучший UX (быстрее результат)
- Риски минимальны


